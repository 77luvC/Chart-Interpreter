{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Time    :   03/16/2025 13:58:00\n",
    "@Author  :   Ran Zhang \n",
    "@Email   :   ran0925@bu.edu / ran.zhang.77@hotmail.com\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/miniconda3/lib/python3.12/site-packages (1.66.3)\n",
      "Requirement already satisfied: gradio in /opt/miniconda3/lib/python3.12/site-packages (5.21.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (3.4.2)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.115.11)\n",
      "Requirement already satisfied: ffmpy in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.2 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (1.7.2)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.29.3)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (2.2.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydub in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.11.0)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.46.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.12/site-packages (from gradio-client==1.7.2->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/miniconda3/lib/python3.12/site-packages (from gradio-client==1.7.2->gradio) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai gradio pandas networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from io import BytesIO\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"Mykey\"\n",
    "client = openai  # Using the openai module as our client\n",
    "chatgpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: initiative insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #   \"Assume the user is a U.S.-based small business owner with at most some college education. \"\n",
    "    #     \"Use plain, easy-to-understand English and avoid technical jargon.\"\n",
    "\n",
    "def get_chatgpt_insight_from_dataurl(data_url):\n",
    "    prompt_text = (\n",
    "        \"what is this chart about? \"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    try:\n",
    "        chat_response = client.chat.completions.create(\n",
    "            model=chatgpt_model,\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        chat_text = chat_response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        chat_text = f\"Error: {e}\"\n",
    "    return chat_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: self-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_insight(insight_text):\n",
    "    eval_prompt = f\"\"\"\n",
    "Please evaluate the following insight according to these criteria. Provide a rating (0 to 5) for each category in JSON format:\n",
    "Accuracy (5: Fully accurate identification of the chart type and key data insights. 3-4: Minor errors but generally correct. 1-2: Significant inaccuracies or missing key details. 0: Completely incorrect),\n",
    "Relevance (5: Highlights the most critical insights with no unnecessary details. 3-4: Covers relevant points but includes minor off-topic information. 1-2: Misses key insights or includes too much irrelevant detail. 0: Completely irrelevant),\n",
    "Clarity (5: Very easy to understand with no jargon. 3-4: Mostly clear but has some complex wording. 1-2: Hard to understand due to technical terms or vague phrasing. 0: Unclear or confusing),\n",
    "Actionability (5: Clear, practical actions that are directly useful. 3-4: Somewhat useful but could be more specific. 1-2: Actions are too vague or impractical. 0: No actionable insights),\n",
    "Conciseness (5: Under 50 words, concise, and to the point. 3-4: Slightly over 50 words but still clear. 1-2: Too wordy or contains unnecessary details. 0: Far exceeds word limit or is too brief to be meaningful).\n",
    "\n",
    "Insight: {insight_text}\n",
    "\n",
    "Please output the result as:\n",
    "{{\"Accuracy\": x, \"Relevance\": x, \"Clarity\": x, \"Actionability\": x, \"Conciseness\": x}}\n",
    "    \"\"\"\n",
    "    chat_messages = [{\"role\": \"user\", \"content\": eval_prompt}]\n",
    "    try:\n",
    "        chat_response = client.chat.completions.create(\n",
    "            model=chatgpt_model,\n",
    "            messages=chat_messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        eval_text = chat_response.choices[0].message.content.strip()\n",
    "        # Clean markdown formatting if present:\n",
    "        eval_text_clean = re.sub(r\"^```(?:json)?\\s*\", \"\", eval_text).rstrip(\"`\").strip()\n",
    "        eval_scores = json.loads(eval_text_clean)\n",
    "    except Exception as e:\n",
    "        print(\"Error in evaluation:\", e)\n",
    "        eval_scores = {}\n",
    "    return eval_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_insight(insight_text):\n",
    "    eval_prompt = f\"\"\"\n",
    "Please evaluate the following insight according to these criteria. Provide a rating (0 to 5) for each category in JSON format:\n",
    "Accuracy (5: Fully accurate identification of the chart type and key data insights. 3-4: Minor errors but generally correct. 1-2: Significant inaccuracies or missing key details. 0: Completely incorrect),\n",
    "Relevance (5: Highlights the most critical insights with no unnecessary details. 3-4: Covers relevant points but includes minor off-topic information. 1-2: Misses key insights or includes too much irrelevant detail. 0: Completely irrelevant),\n",
    "Clarity (5: Very easy to understand with no jargon. 3-4: Mostly clear but has some complex wording. 1-2: Hard to understand due to technical terms or vague phrasing. 0: Unclear or confusing),\n",
    "Actionability (5: Clear, practical actions that are directly useful. 3-4: Somewhat useful but could be more specific. 1-2: Actions are too vague or impractical. 0: No actionable insights),\n",
    "Conciseness (5: Under 50 words, concise, and to the point. 3-4: Slightly over 50 words but still clear. 1-2: Too wordy or contains unnecessary details. 0: Far exceeds word limit or is too brief to be meaningful).\n",
    "\n",
    "Insight: {insight_text}\n",
    "\n",
    "Please output the result as:\n",
    "{{\"Accuracy\": x, \"Relevance\": x, \"Clarity\": x, \"Actionability\": x, \"Conciseness\": x}}\n",
    "    \"\"\"\n",
    "    chat_messages = [{\"role\": \"user\", \"content\": eval_prompt}]\n",
    "    try:\n",
    "        chat_response = client.chat.completions.create(\n",
    "            model=chatgpt_model,\n",
    "            messages=chat_messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        eval_text = chat_response.choices[0].message.content.strip()\n",
    "        # Clean markdown formatting if present:\n",
    "        eval_text_clean = re.sub(r\"^```(?:json)?\\s*\", \"\", eval_text).rstrip(\"`\").strip()\n",
    "        eval_scores = json.loads(eval_text_clean)\n",
    "    except Exception as e:\n",
    "        print(\"Error in evaluation:\", e)\n",
    "        eval_scores = {}\n",
    "    return eval_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: refine prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_prompt(insight_text, eval_scores):\n",
    "    feedback = []\n",
    "    if eval_scores.get(\"Accuracy\", 5) < 5:\n",
    "        feedback.append(\"improve the accuracy by better identifying the chart type and key data insights\")\n",
    "    if eval_scores.get(\"Relevance\", 5) < 5:\n",
    "        feedback.append(\"focus more on the most critical insights and remove unnecessary details\")\n",
    "    if eval_scores.get(\"Clarity\", 5) < 5:\n",
    "        feedback.append(\"simplify the language to improve clarity and avoid technical jargon\")\n",
    "    if eval_scores.get(\"Actionability\", 5) < 5:\n",
    "        feedback.append(\"suggest clearer, more practical and actionable item\")\n",
    "    if eval_scores.get(\"Conciseness\", 5) < 5:\n",
    "        feedback.append(\"make the response less than 50 words, keep this critiria as the first priority\")\n",
    "    if not feedback:\n",
    "        return insight_text\n",
    "    feedback_text = \"; \".join(feedback)\n",
    "    new_prompt = f\"Based on the following feedback: {feedback_text}, please refine the following insight: {insight_text}\"\n",
    "    return new_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: threshold & iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_insight_process(data_url):\n",
    "    max_iterations = 5\n",
    "    target_avg_score = 4.5\n",
    "    eval_records = []\n",
    "    current_insight = get_chatgpt_insight_from_dataurl(data_url)\n",
    "    print(\"Initial Insight:\", current_insight)\n",
    "    eval_records.append({\n",
    "        \"Iteration\": 0,\n",
    "        \"Insight\": current_insight,\n",
    "        \"Average\": None,\n",
    "        \"Accuracy\": None,\n",
    "        \"Relevance\": None,\n",
    "        \"Clarity\": None,\n",
    "        \"Actionability\": None,\n",
    "        \"Conciseness\": None\n",
    "    })\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"\\n--- Iteration {iteration} ---\")\n",
    "        eval_scores = evaluate_insight(current_insight)\n",
    "        if not eval_scores:\n",
    "            print(\"Evaluation failed. Exiting iteration.\")\n",
    "            break\n",
    "        avg_score = sum(eval_scores.values()) / len(eval_scores)\n",
    "        print(\"Evaluation Scores:\")\n",
    "        for key, value in eval_scores.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(\"Average Score:\", avg_score)\n",
    "        record = {\"Iteration\": iteration, \"Insight\": current_insight, \"Average\": avg_score}\n",
    "        record.update(eval_scores)\n",
    "        eval_records.append(record)\n",
    "        if avg_score >= target_avg_score:\n",
    "            print(\"Target average score reached. Stopping iterations.\")\n",
    "            break\n",
    "        refinement_prompt = refine_prompt(current_insight, eval_scores)\n",
    "        print(\"Refinement Prompt:\", refinement_prompt)\n",
    "        chat_messages = [{\"role\": \"user\", \"content\": refinement_prompt}]\n",
    "        try:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=chat_messages,\n",
    "                temperature=0\n",
    "            )\n",
    "            current_insight = chat_response.choices[0].message.content.strip()\n",
    "            print(\"New Insight:\", current_insight)\n",
    "        except Exception as e:\n",
    "            print(\"Error during refinement:\", e)\n",
    "            break\n",
    "    \n",
    "    final_eval_scores = evaluate_insight(current_insight)\n",
    "    if final_eval_scores:\n",
    "        final_avg = sum(final_eval_scores.values()) / len(final_eval_scores)\n",
    "        print(\"\\nFinal Evaluation Scores:\")\n",
    "        for key, value in final_eval_scores.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        final_record = {\"Iteration\": \"Final\", \"Insight\": current_insight, \"Average\": final_avg}\n",
    "        final_record.update(final_eval_scores)\n",
    "        eval_records.append(final_record)\n",
    "    else:\n",
    "        print(\"Final evaluation failed, no scores available.\")\n",
    "    \n",
    "    df = pd.DataFrame(eval_records)\n",
    "    print(\"\\nEvaluation DataFrame:\")\n",
    "    print(df)\n",
    "    return current_insight, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f579252051d00991c6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f579252051d00991c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Insight: This chart appears to be a control chart related to packaging performance. The vertical axis represents \"Late Packaging,\" indicating the number of late packaging instances, while the horizontal axis shows \"Total Order,\" which likely refers to the total number of orders processed.\n",
      "\n",
      "The trend line seems to indicate a decrease in late packaging incidents as the total orders increase, suggesting an improvement in the packaging process over time. The red dots represent individual data points, showing the number of late packaging instances corresponding to each total order value. Overall, the chart likely aims to track and analyze the efficiency of the packaging process.\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Evaluation Scores:\n",
      "  Accuracy: 4\n",
      "  Relevance: 4\n",
      "  Clarity: 4\n",
      "  Actionability: 3\n",
      "  Conciseness: 4\n",
      "Average Score: 3.8\n",
      "Refinement Prompt: Based on the following feedback: improve the accuracy by better identifying the chart type and key data insights; focus more on the most critical insights and remove unnecessary details; simplify the language to improve clarity and avoid technical jargon; suggest clearer, more practical and actionable item; make the response less than 50 words, keep this critiria as the first priority, please refine the following insight: This chart appears to be a control chart related to packaging performance. The vertical axis represents \"Late Packaging,\" indicating the number of late packaging instances, while the horizontal axis shows \"Total Order,\" which likely refers to the total number of orders processed.\n",
      "\n",
      "The trend line seems to indicate a decrease in late packaging incidents as the total orders increase, suggesting an improvement in the packaging process over time. The red dots represent individual data points, showing the number of late packaging instances corresponding to each total order value. Overall, the chart likely aims to track and analyze the efficiency of the packaging process.\n",
      "New Insight: This control chart tracks late packaging incidents against total orders. It shows a downward trend in late packaging as orders increase, indicating improved efficiency. Focus on maintaining this trend to enhance packaging performance further.\n",
      "\n",
      "--- Iteration 2 ---\n",
      "Evaluation Scores:\n",
      "  Accuracy: 5\n",
      "  Relevance: 5\n",
      "  Clarity: 5\n",
      "  Actionability: 4\n",
      "  Conciseness: 4\n",
      "Average Score: 4.6\n",
      "Target average score reached. Stopping iterations.\n",
      "\n",
      "Final Evaluation Scores:\n",
      "  Accuracy: 5\n",
      "  Relevance: 5\n",
      "  Clarity: 5\n",
      "  Actionability: 4\n",
      "  Conciseness: 4\n",
      "\n",
      "Evaluation DataFrame:\n",
      "  Iteration                                            Insight  Average  \\\n",
      "0         0  This chart appears to be a control chart relat...      NaN   \n",
      "1         1  This chart appears to be a control chart relat...      3.8   \n",
      "2         2  This control chart tracks late packaging incid...      4.6   \n",
      "3     Final  This control chart tracks late packaging incid...      4.6   \n",
      "\n",
      "   Accuracy  Relevance  Clarity  Actionability  Conciseness  \n",
      "0       NaN        NaN      NaN            NaN          NaN  \n",
      "1       4.0        4.0      4.0            3.0          4.0  \n",
      "2       5.0        5.0      5.0            4.0          4.0  \n",
      "3       5.0        5.0      5.0            4.0          4.0  \n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "def process_chart(image):\n",
    "    \"\"\"\n",
    "    Convert the uploaded image to a data URL, run the iterative insight process,\n",
    "    and return a summary string containing the lowest-scored insight,\n",
    "    the highest-scored insight, and the final insight.\n",
    "    \"\"\"\n",
    "    # Convert the uploaded PIL image to a data URL\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_bytes = buffered.getvalue()\n",
    "    b64_image = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "    mime_type = \"image/png\"  # Adjust if your image is in a different format\n",
    "    data_url = f\"data:{mime_type};base64,{b64_image}\"\n",
    "    \n",
    "    # Run the iterative process to get the final insight and evaluation DataFrame\n",
    "    final_insight, eval_df = iterative_insight_process(data_url)\n",
    "    \n",
    "    # Filter out records with valid average scores (ignore those with None)\n",
    "    df_valid = eval_df[eval_df[\"Average\"].notnull()]\n",
    "    if not df_valid.empty:\n",
    "        # Find the record with the lowest average score\n",
    "        lowest_row = df_valid.loc[df_valid[\"Average\"].idxmin()]\n",
    "        # Find the record with the highest average score\n",
    "        highest_row = df_valid.loc[df_valid[\"Average\"].idxmax()]\n",
    "        \n",
    "        lowest_str = (f\"Lowest Score Insight (Score: {lowest_row['Average']:.2f}):\\n\"\n",
    "                      f\"{lowest_row['Insight']}\\n\")\n",
    "        highest_str = (f\"Highest Score Insight (Score: {highest_row['Average']:.2f}):\\n\"\n",
    "                       f\"{highest_row['Insight']}\\n\")\n",
    "    else:\n",
    "        lowest_str = \"No evaluation data available for lowest score.\"\n",
    "        highest_str = \"No evaluation data available for highest score.\"\n",
    "    \n",
    "    # Combine the results into a single output string\n",
    "    result = f\"{lowest_str}\\n{highest_str}\\nFinal Insight:\\n{final_insight}\"\n",
    "    return result\n",
    "\n",
    "# Define the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_chart,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload Chart Image\"),\n",
    "    outputs=gr.Textbox(label=\"Insight Summary\"),\n",
    "    title=\"AI-Powered Chart Insight Agent\",\n",
    "    description=(\n",
    "        \"Upload your chart image to receive AI-generated business insights along with evaluation metrics. \"\n",
    "        \"The system iteratively refines its output and displays the lowest-scored and highest-scored insights \"\n",
    "        \"to illustrate improvement over iterations, as well as the final refined insight.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
